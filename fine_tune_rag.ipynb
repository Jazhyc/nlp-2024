{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "import faiss\n",
    "import datasets\n",
    "import torch\n",
    "import typing\n",
    "\n",
    "from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration, RagConfig, AutoConfig, AutoModel, \\\n",
    "    RagTokenizer, BartForConditionalGeneration, AlbertModel\n",
    "\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?id=18xMA2wGPDXArwLyVWN3HXQaF0XnjtugF\"\n",
    "filepath = \"data/gold\"\n",
    "\n",
    "# Check if index exists\n",
    "if os.path.isfile(filepath + \"/index.faiss\"):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    \n",
    "    # Download zip file using gdown\n",
    "    gdown.download(url, \"index.zip\", quiet=False)\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    # Unzip file\n",
    "    with zipfile.ZipFile(\"index.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(filepath)\n",
    "\n",
    "    # Remove zip file\n",
    "    os.remove(\"index.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_name = \"sentence-transformers/paraphrase-albert-base-v2\"\n",
    "encoder_model_type = \"albert\"\n",
    "encoder_config = AutoConfig.from_pretrained(encoder_model_name, output_hidden_states=True)\n",
    "\n",
    "generator_model_name = \"facebook/bart-base\"\n",
    "generator_model_type = \"bart\"\n",
    "generator_config = AutoConfig.from_pretrained(generator_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_config = RagConfig(\n",
    "    question_encoder={\n",
    "        \"model_type\": encoder_model_type,\n",
    "        \"config\": encoder_config,\n",
    "    },\n",
    "    generator = {\n",
    "        \"model_type\": generator_model_type,\n",
    "        \"config\": generator_config\n",
    "    },\n",
    "    index_name=\"custom\",\n",
    "    passages_path=filepath + \"/dataset\",\n",
    "    index_path=filepath + \"/index.faiss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RagRetriever(\n",
    "    config=rag_config,\n",
    "    question_encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_model_name),\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQuestionEncoder(AlbertModel):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Call the original forward method\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "\n",
    "        attention_mask = kwargs.get('attention_mask', None)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            # Assume all 1s if not given, use output to get mask. The final output must be two-dimensional\n",
    "            attention_mask = torch.ones(outputs[0].shape[:2], device=outputs[0].device)\n",
    "            \n",
    "\n",
    "        token_embeddings = outputs[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        pooler_output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        # Return pooler output, hidden states and attentions\n",
    "        return BaseModelOutputWithPooling(pooler_output=pooler_output, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n",
    "\n",
    "\n",
    "# Use the custom question encoder\n",
    "question_encoder_model = CustomQuestionEncoder.from_pretrained(encoder_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rag_model = RagSequenceForGeneration(\n",
    "    config=rag_config,\n",
    "    retriever=retriever,\n",
    "    question_encoder=question_encoder_model,\n",
    "    generator=BartForConditionalGeneration.from_pretrained(generator_model_name),\n",
    ")\n",
    "\n",
    "rag_tokenizer = RagTokenizer(\n",
    "    question_encoder=AutoTokenizer.from_pretrained(encoder_model_name),\n",
    "    generator=AutoTokenizer.from_pretrained(generator_model_name),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:590: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of the Netherlands\n",
      "Answer: Netherlands / The Netherlands, informally Holland, is a country located in northwestern Europe with overseas territories in the Caribbean. It is the largest of the four constituent countries of the Kingdom of the Netherlands. The Netherlands consists of twelve provinces;\n"
     ]
    }
   ],
   "source": [
    "rag_model.to(device)\n",
    "\n",
    "question = \"What is the capital of the Netherlands\"\n",
    "inputs = rag_tokenizer.question_encoder(question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated = rag_model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=50, num_beams=4, early_stopping=False)\n",
    "generated_string = rag_tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", generated_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
