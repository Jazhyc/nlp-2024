{"cells":[{"cell_type":"code","execution_count":83,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1712053796476,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"uJLsLXY2RcN7"},"outputs":[],"source":["import gdown\n","import zipfile\n","import os\n","import faiss\n","import datasets\n","import torch\n","import typing\n","from typing import Tuple, Dict\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from datasets import load_dataset\n","import json\n","\n","from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration, RagConfig, AutoConfig, AutoModel, \\\n","    RagTokenizer, BartForConditionalGeneration, AlbertModel, Trainer, TrainingArguments\n","\n","from transformers.modeling_outputs import BaseModelOutputWithPooling"]},{"cell_type":"markdown","metadata":{"id":"SaprgiH5RcN9"},"source":["## Loading the Dataset"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712051649166,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"vlri4n3nRcN-","outputId":"608b3919-c833-41c5-c182-cf4cffeda1bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["File already exists\n"]}],"source":["url = \"https://drive.google.com/uc?id=18xMA2wGPDXArwLyVWN3HXQaF0XnjtugF\"\n","filepath = \"data/gold\"\n","\n","# Check if index exists\n","if os.path.isfile(filepath + \"/index.faiss\"):\n","    print(\"File already exists\")\n","else:\n","\n","    # Download zip file using gdown\n","    gdown.download(url, \"index.zip\", quiet=False)\n","\n","    # Create directory if it doesn't exist\n","    if not os.path.exists(filepath):\n","        os.makedirs(filepath)\n","\n","    # Unzip file\n","    with zipfile.ZipFile(\"index.zip\", 'r') as zip_ref:\n","        zip_ref.extractall(filepath)\n","\n","    # Remove zip file\n","    os.remove(\"index.zip\")"]},{"cell_type":"markdown","metadata":{"id":"zPkE2Q3vRcN-"},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":843,"status":"ok","timestamp":1712051650005,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"XTotKdNIRcN_"},"outputs":[],"source":["encoder_model_name = \"sentence-transformers/paraphrase-albert-base-v2\"\n","encoder_model_type = \"albert\"\n","encoder_config = AutoConfig.from_pretrained(encoder_model_name, output_hidden_states=True)\n","\n","generator_model_name = \"facebook/bart-base\"\n","generator_model_type = \"bart\"\n","generator_config = AutoConfig.from_pretrained(generator_model_name)"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712051650005,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"dOyr7NreRcN_"},"outputs":[],"source":["rag_config = RagConfig(\n","    question_encoder={\n","        \"model_type\": encoder_model_type,\n","        \"config\": encoder_config,\n","    },\n","    generator = {\n","        \"model_type\": generator_model_type,\n","        \"config\": generator_config\n","    },\n","    index_name=\"custom\",\n","    passages_path=filepath + \"/dataset\",\n","    index_path=filepath + \"/index.faiss\",\n",")"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":2166,"status":"ok","timestamp":1712051652168,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"urZEz5xsRcN_"},"outputs":[],"source":["\n","retriever = RagRetriever(\n","    config=rag_config,\n","    question_encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_model_name),\n","    generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name),\n",")"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712051652168,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"-4bA0B_PRcN_"},"outputs":[],"source":["class CustomQuestionEncoder(AlbertModel):\n","    def forward(self, *args, **kwargs):\n","        # Call the original forward method\n","        outputs = super().forward(*args, **kwargs)\n","        attention_mask = kwargs.get('attention_mask', None)\n","\n","        if attention_mask is None:\n","            # Assume all 1s if not given, use output to get mask. The final output must be two-dimensional\n","            attention_mask = torch.ones(outputs[0].shape[:2], device=outputs[0].device)\n","\n","\n","        token_embeddings = outputs[0] #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        pooler_output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","        # Return pooler output, hidden states and attentions\n","        return BaseModelOutputWithPooling(pooler_output=pooler_output, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n","\n","    # Fine tuning code\n","    def _step(self, batch: dict) -> Tuple:\n","        source_ids, source_mask, target_ids = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"decoder_input_ids\"]\n","\n","        assert self.is_rag_model\n","        generator = self.model.rag.generator\n","        decoder_input_ids = target_ids\n","        lm_labels = decoder_input_ids\n","        rag_kwargs[\"reduce_loss\"] = True\n","\n","        assert decoder_input_ids is not None\n","\n","        outputs = self(\n","            source_ids,\n","            attention_mask=source_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            use_cache=False,\n","            labels=lm_labels,\n","            **rag_kwargs,\n","        )\n","\n","        loss = outputs[\"loss\"]\n","        return (loss,)\n","\n","    def training_step(self, batch, batch_idx) -> Dict:\n","        loss_tensors = self._step(batch)\n","\n","        return {\"loss\": loss_tensors[0]}\n","\n","    def validation_step(self, batch, batch_idx) -> Dict:\n","        return self._generative_step(batch)\n","\n","    def validation_epoch_end(self, outputs, prefix=\"val\") -> Dict:\n","        self.step_count += 1\n","        losses = {k: torch.stack([x[k] for x in outputs]).mean() for k in self.loss_names}\n","        loss = losses[\"loss\"]\n","        preds = flatten_list([x[\"preds\"] for x in outputs])\n","        return {\"preds\": preds, f\"{prefix}_loss\": loss}\n","\n","    def test_step(self, batch, batch_idx):\n","        return self._generative_step(batch)\n","\n","    def test_epoch_end(self, outputs):\n","        return self.validation_epoch_end(outputs, prefix=\"test\")\n","\n","    def _generative_step(self, batch: dict) -> dict:\n","        batch = BatchEncoding(batch).to(device=self.model.device)\n","        generated_ids = self.model.generate(\n","            batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            do_deduplication=False,  # rag specific parameter\n","            use_cache=True,\n","            min_length=1,\n","            max_length=self.target_lens[\"val\"],\n","        )\n","\n","        preds: List[str] = self.ids_to_clean_text(generated_ids)\n","        target: List[str] = self.ids_to_clean_text(batch[\"decoder_input_ids\"])\n","        loss_tensors = self._step(batch)\n","\n","        summ_len = np.mean(lmap(len, generated_ids))\n","        return summ_len\n","\n","\n","\n","\n","# Use the custom question encoder\n","question_encoder_model = CustomQuestionEncoder.from_pretrained(encoder_model_name)"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":3400,"status":"ok","timestamp":1712051655565,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"1KC2tGKZRcOA"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","rag_model = RagSequenceForGeneration(\n","    config=rag_config,\n","    retriever=retriever,\n","    question_encoder=question_encoder_model,\n","    generator=BartForConditionalGeneration.from_pretrained(generator_model_name),\n",")\n","\n","rag_tokenizer = RagTokenizer(\n","    question_encoder=AutoTokenizer.from_pretrained(encoder_model_name),\n","    generator=AutoTokenizer.from_pretrained(generator_model_name),\n",")"]},{"cell_type":"markdown","metadata":{"id":"_ghxOByYTu7y"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":152,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1712057412643,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"LGRyPkj-P27l"},"outputs":[],"source":["# Create pytorch dataset\n","class QuestionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","        # Get max length from tokenizer\n","        self.max_length = 512\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        question = self.data.iloc[idx]['question']\n","        question_encoding = self.tokenizer.question_encoder(question, return_tensors=\"pt\")\n","\n","        return {**question_encoding}"]},{"cell_type":"code","execution_count":146,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3287,"status":"ok","timestamp":1712057297153,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"XHqGOOGGfSWe","outputId":"3847381c-3795-4ade-ef21-1ff5664ff431"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'url': 'http://www.freebase.com/view/en/justin_bieber', 'question': 'what is the name of justin bieber brother?', 'answers': ['Jazmyn Bieber', 'Jaxon Bieber']}\n"]}],"source":["\n","\n","# Example: Load a dataset\n","questions_dataset = load_dataset(\"web_questions\")\n","\n","# Accessing data\n","print(questions_dataset[\"train\"][0])\n","questions_dataset\n","\n","\n","# Use pd.json_normalize to convert the JSON to a DataFrame\n","questions_df = pd.json_normalize(questions_dataset[\"train\"], meta=['url','question', 'answers'])\n","\n","# Split the dataset into training and validation\n","train_df, val_df = train_test_split(questions_df, test_size=0.3)"]},{"cell_type":"code","execution_count":153,"metadata":{"executionInfo":{"elapsed":555,"status":"ok","timestamp":1712057415704,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"b_Jy9hbfjFJd"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n","train_dataset = QuestionDataset(train_df, rag_tokenizer)\n","val_dataset = QuestionDataset(val_df, rag_tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"erLTQZwuRgYX"},"source":["Fine tuning"]},{"cell_type":"code","execution_count":154,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":807,"status":"error","timestamp":1712057418067,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"FuYcBGz3g7nJ","outputId":"7b9cb58f-40c7-4b8c-b377-18ee15f03b7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]},{"ename":"ValueError","evalue":"The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: args,kwargs,label,label_ids.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-154-5facd07d4826>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1625\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \"\"\"\n\u001b[1;32m   2894\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2847\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2849\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2850\u001b[0m                 \u001b[0;34m\"The batch received was empty, your model won't be able to train on it. Double-check that your \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m                 \u001b[0;34mf\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: args,kwargs,label,label_ids."]}],"source":["# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"trainer\",\n","    per_device_train_batch_size=4,\n","    num_train_epochs=3,\n","    evaluation_strategy=\"epoch\",\n",")\n","\n","# Instantiate the Trainer\n","trainer = Trainer(\n","    model=question_encoder_model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=train_dataset,\n",")\n","\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"VBmdTUPeRcOA"},"source":["## Testing the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8mEeF_rRcOA","outputId":"d83c7b3e-23c8-4d25-864c-0496a17b214d"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\User\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:590: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"]},{"name":"stdout","output_type":"stream","text":["Question: What is the capital of the Netherlands\n","Answer: Netherlands / The Netherlands, informally Holland, is a country located in northwestern Europe with overseas territories in the Caribbean. It is the largest of the four constituent countries of the Kingdom of the Netherlands. The Netherlands consists of twelve provinces;\n"]}],"source":["rag_model.to(device)\n","\n","question = \"What is the capital of the Netherlands\"\n","inputs = rag_tokenizer.question_encoder(question, return_tensors=\"pt\").to(device)\n","\n","generated = rag_model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=50, num_beams=4, early_stopping=False)\n","generated_string = rag_tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n","\n","print(\"Question:\", question)\n","print(\"Answer:\", generated_string)"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2464,"status":"ok","timestamp":1712051823950,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"w3uk_Y5FPV_W","outputId":"627be87f-5297-4846-af22-0978051ac833"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'url': 'http://www.freebase.com/view/en/justin_bieber', 'question': 'what is the name of justin bieber brother?', 'answers': ['Jazmyn Bieber', 'Jaxon Bieber']}\n"]},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['url', 'question', 'answers'],\n","        num_rows: 3778\n","    })\n","    test: Dataset({\n","        features: ['url', 'question', 'answers'],\n","        num_rows: 2032\n","    })\n","})"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
