{"cells":[{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1712053796476,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"uJLsLXY2RcN7"},"outputs":[],"source":["import gdown\n","import zipfile\n","import os\n","import faiss\n","import datasets\n","import torch\n","import typing\n","from typing import Tuple, Dict\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from datasets import load_dataset\n","import json\n","\n","from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration, RagConfig, AutoConfig, AutoModel, \\\n","    RagTokenizer, BartForConditionalGeneration, AlbertModel, Trainer, TrainingArguments\n","\n","from transformers.modeling_outputs import BaseModelOutputWithPooling\n","\n","from reqs.lightning_base import BaseTransformer\n","from reqs.utils import parse_sh_args"]},{"cell_type":"markdown","metadata":{"id":"SaprgiH5RcN9"},"source":["## Loading the Dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712051649166,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"vlri4n3nRcN-","outputId":"608b3919-c833-41c5-c182-cf4cffeda1bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["File already exists\n"]}],"source":["url = \"https://drive.google.com/uc?id=18xMA2wGPDXArwLyVWN3HXQaF0XnjtugF\"\n","filepath = \"data/gold\"\n","\n","# Check if index exists\n","if os.path.isfile(filepath + \"/index.faiss\"):\n","    print(\"File already exists\")\n","else:\n","\n","    # Download zip file using gdown\n","    gdown.download(url, \"index.zip\", quiet=False)\n","\n","    # Create directory if it doesn't exist\n","    if not os.path.exists(filepath):\n","        os.makedirs(filepath)\n","\n","    # Unzip file\n","    with zipfile.ZipFile(\"index.zip\", 'r') as zip_ref:\n","        zip_ref.extractall(filepath)\n","\n","    # Remove zip file\n","    os.remove(\"index.zip\")"]},{"cell_type":"markdown","metadata":{"id":"zPkE2Q3vRcN-"},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":843,"status":"ok","timestamp":1712051650005,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"XTotKdNIRcN_"},"outputs":[],"source":["encoder_model_name = \"sentence-transformers/paraphrase-albert-base-v2\"\n","encoder_model_type = \"albert\"\n","encoder_config = AutoConfig.from_pretrained(encoder_model_name, output_hidden_states=True)\n","\n","generator_model_name = \"facebook/bart-base\"\n","generator_model_type = \"bart\"\n","generator_config = AutoConfig.from_pretrained(generator_model_name)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712051650005,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"dOyr7NreRcN_"},"outputs":[],"source":["rag_config = RagConfig(\n","    question_encoder={\n","        \"model_type\": encoder_model_type,\n","        \"config\": encoder_config,\n","    },\n","    generator = {\n","        \"model_type\": generator_model_type,\n","        \"config\": generator_config\n","    },\n","    index_name=\"custom\",\n","    passages_path=filepath + \"/dataset\",\n","    index_path=filepath + \"/index.faiss\",\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":2166,"status":"ok","timestamp":1712051652168,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"urZEz5xsRcN_"},"outputs":[],"source":["\n","rag_retriever = RagRetriever(\n","    config=rag_config,\n","    question_encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_model_name),\n","    generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name),\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712051652168,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"-4bA0B_PRcN_"},"outputs":[],"source":["class CustomQuestionEncoder(AlbertModel):\n","    def forward(self, *args, **kwargs):\n","        # Call the original forward method\n","        outputs = super().forward(*args, **kwargs)\n","        attention_mask = kwargs.get('attention_mask', None)\n","\n","        if attention_mask is None:\n","            # Assume all 1s if not given, use output to get mask. The final output must be two-dimensional\n","            attention_mask = torch.ones(outputs[0].shape[:2], device=outputs[0].device)\n","\n","\n","        token_embeddings = outputs[0] #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        pooler_output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","        # Return pooler output, hidden states and attentions\n","        return BaseModelOutputWithPooling(pooler_output=pooler_output, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n","\n","    # # Fine tuning code\n","    # def _step(self, batch: dict) -> Tuple:\n","    #     source_ids, source_mask, target_ids = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"decoder_input_ids\"]\n","\n","    #     assert self.is_rag_model\n","    #     generator = self.model.rag.generator\n","    #     decoder_input_ids = target_ids\n","    #     lm_labels = decoder_input_ids\n","    #     rag_kwargs[\"reduce_loss\"] = True\n","\n","    #     assert decoder_input_ids is not None\n","\n","    #     outputs = self(\n","    #         source_ids,\n","    #         attention_mask=source_mask,\n","    #         decoder_input_ids=decoder_input_ids,\n","    #         use_cache=False,\n","    #         labels=lm_labels,\n","    #         **rag_kwargs,\n","    #     )\n","\n","    #     loss = outputs[\"loss\"]\n","    #     return (loss,)\n","\n","    # def training_step(self, batch, batch_idx) -> Dict:\n","    #     loss_tensors = self._step(batch)\n","\n","    #     return {\"loss\": loss_tensors[0]}\n","\n","    # def validation_step(self, batch, batch_idx) -> Dict:\n","    #     return self._generative_step(batch)\n","\n","    # def validation_epoch_end(self, outputs, prefix=\"val\") -> Dict:\n","    #     self.step_count += 1\n","    #     losses = {k: torch.stack([x[k] for x in outputs]).mean() for k in self.loss_names}\n","    #     loss = losses[\"loss\"]\n","    #     preds = flatten_list([x[\"preds\"] for x in outputs])\n","    #     return {\"preds\": preds, f\"{prefix}_loss\": loss}\n","\n","    # def test_step(self, batch, batch_idx):\n","    #     return self._generative_step(batch)\n","\n","    # def test_epoch_end(self, outputs):\n","    #     return self.validation_epoch_end(outputs, prefix=\"test\")\n","\n","    # def _generative_step(self, batch: dict) -> dict:\n","    #     batch = BatchEncoding(batch).to(device=self.model.device)\n","    #     generated_ids = self.model.generate(\n","    #         batch[\"input_ids\"],\n","    #         attention_mask=batch[\"attention_mask\"],\n","    #         do_deduplication=False,  # rag specific parameter\n","    #         use_cache=True,\n","    #         min_length=1,\n","    #         max_length=self.target_lens[\"val\"],\n","    #     )\n","\n","    #     preds: List[str] = self.ids_to_clean_text(generated_ids)\n","    #     target: List[str] = self.ids_to_clean_text(batch[\"decoder_input_ids\"])\n","    #     loss_tensors = self._step(batch)\n","\n","    #     summ_len = np.mean(lmap(len, generated_ids))\n","    #     return summ_len\n","\n","\n","# Use the custom question encoder\n","question_encoder_model = CustomQuestionEncoder.from_pretrained(encoder_model_name)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3400,"status":"ok","timestamp":1712051655565,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"1KC2tGKZRcOA"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","rag_model = RagSequenceForGeneration(\n","    config=rag_config,\n","    retriever=rag_retriever,\n","    question_encoder=question_encoder_model,\n","    generator=BartForConditionalGeneration.from_pretrained(generator_model_name),\n",")\n","\n","rag_tokenizer = RagTokenizer(\n","    question_encoder=AutoTokenizer.from_pretrained(encoder_model_name),\n","    generator=AutoTokenizer.from_pretrained(generator_model_name),\n",")"]},{"cell_type":"markdown","metadata":{"id":"_ghxOByYTu7y"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1712057412643,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"LGRyPkj-P27l"},"outputs":[],"source":["# Create pytorch dataset\n","class QuestionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","        # Get max length from tokenizer\n","        self.max_length = 512\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        question = self.data.iloc[idx]['question']\n","        question_encoding = self.tokenizer.question_encoder(question, return_tensors=\"pt\")\n","\n","        return {**question_encoding}"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3287,"status":"ok","timestamp":1712057297153,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"XHqGOOGGfSWe","outputId":"3847381c-3795-4ade-ef21-1ff5664ff431"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c080cb49f4e241bca34ad12a40fb2e22","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'qText': 'what is the name of justin bieber brother?', 'qId': 'wqr000000', 'answers': ['Jazmyn Bieber', 'Jaxon Bieber']}\n"]}],"source":["\n","\n","# Example: Load a dataset\n","questions_dataset = load_dataset(\"web_questions\")\n","\n","# Accessing data\n","print(questions_dataset[\"train\"][0])\n","questions_dataset\n","\n","\n","# Use pd.json_normalize to convert the JSON to a DataFrame\n","questions_df = pd.json_normalize(questions_dataset[\"train\"], meta=['url','question', 'answers'])\n","\n","# Split the dataset into training and validation\n","train_df, val_df = train_test_split(questions_df, test_size=0.3)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":555,"status":"ok","timestamp":1712057415704,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"b_Jy9hbfjFJd"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n","train_dataset = QuestionDataset(train_df, rag_tokenizer)\n","val_dataset = QuestionDataset(val_df, rag_tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"erLTQZwuRgYX"},"source":["Fine tuning"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":807,"status":"error","timestamp":1712057418067,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"FuYcBGz3g7nJ","outputId":"7b9cb58f-40c7-4b8c-b377-18ee15f03b7e"},"outputs":[{"ename":"ImportError","evalue":"Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Instantiate the Trainer\u001b[39;00m\n\u001b[0;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39mquestion_encoder_model,\n\u001b[0;32m     12\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     13\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     14\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     15\u001b[0m )\n","File \u001b[1;32m<string>:123\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha)\u001b[0m\n","File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\training_args.py:1528\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1523\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1531\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1532\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1533\u001b[0m ):\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1535\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1536\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1537\u001b[0m     )\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1549\u001b[0m ):\n","File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\training_args.py:1995\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1993\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1994\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n","File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:56\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     54\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n","File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\training_args.py:1905\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 1905\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1906\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1907\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1908\u001b[0m         )\n\u001b[0;32m   1909\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"]}],"source":["# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"trainer\",\n","    per_device_train_batch_size=4,\n","    num_train_epochs=3,\n","    evaluation_strategy=\"epoch\",\n",")\n","\n","# Instantiate the Trainer\n","trainer = Trainer(\n","    model=question_encoder_model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=train_dataset,\n",")\n","\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{"id":"VBmdTUPeRcOA"},"source":["## Testing the Model"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"S8mEeF_rRcOA","outputId":"d83c7b3e-23c8-4d25-864c-0496a17b214d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What is the capital of the Netherlands\n","Answer: Netherlands / The Netherlands, informally Holland, is a country located in northwestern Europe with overseas territories in the Caribbean. It is the largest of the four constituent countries of the Kingdom of the Netherlands. The Netherlands consists of twelve provinces;\n"]}],"source":["rag_model.to(device)\n","\n","question = \"What is the capital of the Netherlands\"\n","inputs = rag_tokenizer.question_encoder(question, return_tensors=\"pt\").to(device)\n","\n","generated = rag_model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=50, num_beams=4, early_stopping=False)\n","generated_string = rag_tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n","\n","print(\"Question:\", question)\n","print(\"Answer:\", generated_string)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2464,"status":"ok","timestamp":1712051823950,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"w3uk_Y5FPV_W","outputId":"627be87f-5297-4846-af22-0978051ac833"},"outputs":[],"source":["# Adapted from https://github.com/huggingface/transformers/blob/main/examples/research_projects/rag/finetune_rag.py#L97\n","# We make abuse of global variables here to make the code simpler\n","\n","class GenerativeQAModule(BaseTransformer):\n","    mode = \"generative_qa\"\n","    loss_names = [\"loss\"]\n","    metric_names = [\"em\"] # exact match\n","    val_metric = \"em\"\n","\n","    def __init__(self, hparams, **kwargs):\n","\n","        self.retriever = rag_retriever\n","        prefix = rag_config.question_encoder.prefix\n","\n","        super().__init__(hparams, config=rag_config, tokenizer=rag_tokenizer, model=rag_model)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["hparams = parse_sh_args(\"fine_tune_rag.sh\")\n","QAModule = GenerativeQAModule(hparams)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
