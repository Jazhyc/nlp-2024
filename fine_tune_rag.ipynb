{"cells":[{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1712053796476,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"uJLsLXY2RcN7"},"outputs":[],"source":["import gdown\n","import zipfile\n","import os\n","import faiss\n","import datasets\n","\n","import typing\n","import time\n","from typing import Tuple, Dict\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from datasets import load_dataset\n","import json\n","import numpy as np\n","\n","from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration, RagConfig, AutoConfig, AutoModel, \\\n","    RagTokenizer, BartForConditionalGeneration, AlbertModel, Trainer, TrainingArguments, BatchEncoding\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","from transformers.modeling_outputs import BaseModelOutputWithPooling\n","\n","from reqs.lightning_base import BaseTransformer\n","from reqs.utils import *"]},{"cell_type":"markdown","metadata":{"id":"SaprgiH5RcN9"},"source":["## Loading the Dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712051649166,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"vlri4n3nRcN-","outputId":"608b3919-c833-41c5-c182-cf4cffeda1bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["File already exists\n"]}],"source":["url = \"https://drive.google.com/uc?id=18xMA2wGPDXArwLyVWN3HXQaF0XnjtugF\"\n","filepath = \"data/gold\"\n","\n","# Check if index exists\n","if os.path.isfile(filepath + \"/index.faiss\"):\n","    print(\"File already exists\")\n","else:\n","\n","    # Download zip file using gdown\n","    gdown.download(url, \"index.zip\", quiet=False)\n","\n","    # Create directory if it doesn't exist\n","    if not os.path.exists(filepath):\n","        os.makedirs(filepath)\n","\n","    # Unzip file\n","    with zipfile.ZipFile(\"index.zip\", 'r') as zip_ref:\n","        zip_ref.extractall(filepath)\n","\n","    # Remove zip file\n","    os.remove(\"index.zip\")"]},{"cell_type":"markdown","metadata":{"id":"zPkE2Q3vRcN-"},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":843,"status":"ok","timestamp":1712051650005,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"XTotKdNIRcN_"},"outputs":[],"source":["encoder_model_name = \"sentence-transformers/paraphrase-albert-base-v2\"\n","encoder_model_type = \"albert\"\n","encoder_config = AutoConfig.from_pretrained(encoder_model_name, output_hidden_states=True)\n","\n","generator_model_name = \"facebook/bart-base\"\n","generator_model_type = \"bart\"\n","generator_config = AutoConfig.from_pretrained(generator_model_name)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712051650005,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"dOyr7NreRcN_"},"outputs":[],"source":["rag_config = RagConfig(\n","    question_encoder={\n","        \"model_type\": encoder_model_type,\n","        \"config\": encoder_config,\n","    },\n","    generator = {\n","        \"model_type\": generator_model_type,\n","        \"config\": generator_config\n","    },\n","    index_name=\"custom\",\n","    passages_path=filepath + \"/dataset\",\n","    index_path=filepath + \"/index.faiss\",\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":2166,"status":"ok","timestamp":1712051652168,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"urZEz5xsRcN_"},"outputs":[],"source":["\n","rag_retriever = RagRetriever(\n","    config=rag_config,\n","    question_encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_model_name),\n","    generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name),\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712051652168,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"-4bA0B_PRcN_"},"outputs":[],"source":["class CustomQuestionEncoder(AlbertModel):\n","    def forward(self, *args, **kwargs):\n","        # Call the original forward method\n","        outputs = super().forward(*args, **kwargs)\n","        attention_mask = kwargs.get('attention_mask', None)\n","\n","        if attention_mask is None:\n","            # Assume all 1s if not given, use output to get mask. The final output must be two-dimensional\n","            attention_mask = torch.ones(outputs[0].shape[:2], device=outputs[0].device)\n","\n","\n","        token_embeddings = outputs[0] #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        pooler_output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","        # Return pooler output, hidden states and attentions\n","        return BaseModelOutputWithPooling(pooler_output=pooler_output, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n","\n","# Use the custom question encoder\n","question_encoder_model = CustomQuestionEncoder.from_pretrained(encoder_model_name)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3400,"status":"ok","timestamp":1712051655565,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"1KC2tGKZRcOA"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","rag_model = RagSequenceForGeneration(\n","    config=rag_config,\n","    retriever=rag_retriever,\n","    question_encoder=question_encoder_model,\n","    generator=BartForConditionalGeneration.from_pretrained(generator_model_name),\n",")\n","\n","rag_tokenizer = RagTokenizer(\n","    question_encoder=AutoTokenizer.from_pretrained(encoder_model_name),\n","    generator=AutoTokenizer.from_pretrained(generator_model_name),\n",")"]},{"cell_type":"markdown","metadata":{"id":"_ghxOByYTu7y"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1712057412643,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"LGRyPkj-P27l"},"outputs":[],"source":["# Create pytorch dataset\n","class QuestionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","        # Get max length from tokenizer\n","        self.max_length = 512\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        question = self.data.iloc[idx]['question']\n","        question_encoding = self.tokenizer.question_encoder(question, return_tensors=\"pt\")\n","\n","        return {**question_encoding}"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3287,"status":"ok","timestamp":1712057297153,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"XHqGOOGGfSWe","outputId":"3847381c-3795-4ade-ef21-1ff5664ff431"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c080cb49f4e241bca34ad12a40fb2e22","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'qText': 'what is the name of justin bieber brother?', 'qId': 'wqr000000', 'answers': ['Jazmyn Bieber', 'Jaxon Bieber']}\n"]}],"source":["\n","\n","# Example: Load a dataset\n","questions_dataset = load_dataset(\"web_questions\")\n","\n","# Accessing data\n","print(questions_dataset[\"train\"][0])\n","questions_dataset\n","\n","\n","# Use pd.json_normalize to convert the JSON to a DataFrame\n","questions_df = pd.json_normalize(questions_dataset[\"train\"], meta=['url','question', 'answers'])\n","\n","# Split the dataset into training and validation\n","train_df, val_df = train_test_split(questions_df, test_size=0.3)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":555,"status":"ok","timestamp":1712057415704,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"b_Jy9hbfjFJd"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n","train_dataset = QuestionDataset(train_df, rag_tokenizer)\n","val_dataset = QuestionDataset(val_df, rag_tokenizer)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"trainer\",\n","    per_device_train_batch_size=4,\n","    num_train_epochs=3,\n","    evaluation_strategy=\"epoch\",\n",")\n","\n","# Instantiate the Trainer\n","trainer = Trainer(\n","    model=question_encoder_model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=train_dataset,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"VBmdTUPeRcOA"},"source":["## Initial Testing"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"S8mEeF_rRcOA","outputId":"d83c7b3e-23c8-4d25-864c-0496a17b214d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What is the capital of the Netherlands\n","Answer: Netherlands / The Netherlands, informally Holland, is a country located in northwestern Europe with overseas territories in the Caribbean. It is the largest of the four constituent countries of the Kingdom of the Netherlands. The Netherlands consists of twelve provinces;\n"]}],"source":["rag_model.to(device)\n","\n","question = \"What is the capital of the Netherlands\"\n","inputs = rag_tokenizer.question_encoder(question, return_tensors=\"pt\").to(device)\n","\n","generated = rag_model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=50, num_beams=4, early_stopping=False)\n","generated_string = rag_tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n","\n","print(\"Question:\", question)\n","print(\"Answer:\", generated_string)"]},{"cell_type":"markdown","metadata":{},"source":["## Fine-tuning Code"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2464,"status":"ok","timestamp":1712051823950,"user":{"displayName":"E.F. Falca","userId":"05442982892297517780"},"user_tz":-120},"id":"w3uk_Y5FPV_W","outputId":"627be87f-5297-4846-af22-0978051ac833"},"outputs":[],"source":["# Adapted from https://github.com/huggingface/transformers/blob/main/examples/research_projects/rag/finetune_rag.py#L97\n","\n","class GenerativeQAModule(BaseTransformer):\n","    mode = \"generative_qa\"\n","    loss_names = [\"loss\"]\n","    metric_names = [\"em\"] # exact match\n","    val_metric = \"em\"\n","\n","    def __init__(self, hparams, **kwargs):\n","        # Global variables used here from simplicity\n","\n","        self.retriever = rag_retriever\n","        prefix = rag_config.question_encoder.prefix\n","\n","        super().__init__(hparams, config=rag_config, tokenizer=rag_tokenizer, model=rag_model)\n","\n","        self.dataset_kwargs = {\n","            \"prefix\": prefix,\n","            \"max_source_length\": self.max_source_length,\n","            \"data_dir\": self.data_dir,\n","        }\n","\n","    def forward(self, input_ids, **kwargs):\n","        return self.model(input_ids, **kwargs)\n","    \n","    def ids_to_clean_text(self, generated_ids):\n","        preds = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","        return lmap(str.strip, preds)\n","    \n","    def _step(self, batch: dict) -> Tuple:\n","        source_ids, source_mask, target_ids = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"decoder_input_ids\"]\n","\n","        rag_kwargs = {}\n","        decoder_input_ids = target_ids\n","        lm_labels = decoder_input_ids\n","        rag_kwargs[\"reduce_loss\"] = True\n","\n","        assert decoder_input_ids is not None\n","\n","        outputs = self(\n","            source_ids,\n","            attention_mask=source_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            use_cache=False,\n","            labels=lm_labels,\n","            **rag_kwargs,\n","        )\n","\n","        loss = outputs[\"loss\"]\n","        return (loss,)\n","    \n","    def training_step(self, batch, batch_idx) -> Dict:\n","        loss_tensors = self._step(batch)\n","\n","        logs = {name: loss.detach() for name, loss in zip(self.loss_names, loss_tensors)}\n","\n","        logs[\"tpb\"] = (\n","            batch[\"input_ids\"].ne(self.tokenizer.question_encoder.pad_token_id).sum() + \n","            batch[\"decoder_input_ids\"].ne(self.tokenizer.generator.pad_token_id).sum()\n","        )\n","\n","        return {\"loss\": loss_tensors[0], \"log\": logs}\n","    \n","    def validation_step(self, batch, batch_idx) -> Dict:\n","        return self._generative_step(batch)\n","    \n","    def validation_epoch_end(self, outputs, prefix=\"val\") -> Dict:\n","        self.step_count += 1\n","        losses = {k: torch.stack([x[k] for x in outputs]).mean() for k in self.loss_names}\n","        loss = losses[\"loss\"]\n","\n","        gen_metrics = {\n","            k: np.array([x[k] for x in outputs]).mean() for k in self.metric_names + [\"gen_time\", \"gen_len\"]\n","        }\n","\n","        metrics_tensor: torch.FloatTensor = torch.tensor(gen_metrics[self.val_metric]).type_as(loss)\n","        gen_metrics.update({k: v.item() for k, v in losses.items()})\n","\n","        losses.update(gen_metrics)\n","        metrics = {f\"{prefix}_avg_{k}\": x for k, x in losses.items()}\n","        preds = flatten_list([x[\"preds\"] for x in outputs])\n","        \n","        return {\"log\": metrics, \"preds\": preds, f\"{prefix}_loss\": loss, f\"{prefix}_{self.val_metric}\": metrics_tensor}\n","    \n","    def calc_generative_metrics(self, preds, target) -> Dict:\n","        return calculate_exact_match(preds, target)\n","    \n","    def _generative_step(self, batch: dict) -> dict:\n","        \n","        start_time = time.time()\n","        batch = BatchEncoding(batch).to(device=self.model.device)\n","        generated_ids = self.model.generate(\n","            batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            do_deduplication=False,  # rag specific parameter\n","            use_cache=True,\n","            min_length=1,\n","            max_length=self.target_lens[\"val\"],\n","        )\n","\n","        gen_time = (time.time() - start_time) / batch[\"input_ids\"].shape[0]\n","        preds: List[str] = self.ids_to_clean_text(generated_ids)\n","        target: List[str] = self.ids_to_clean_text(batch[\"decoder_input_ids\"])\n","        loss_tensors = self._step(batch)\n","        base_metrics = dict(zip(self.loss_names, loss_tensors))\n","        gen_metrics: Dict = self.calc_generative_metrics(preds, target)\n","\n","        summ_len = np.mean(lmap(len, generated_ids))\n","        base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target, **gen_metrics)\n","        return base_metrics\n","    \n","    def test_step(self, batch, batch_idx):\n","        return self._generative_step(batch)\n","\n","    def test_epoch_end(self, outputs):\n","        return self.validation_epoch_end(outputs, prefix=\"test\")\n","    \n","    def get_dataset(self, type_path) -> Seq2SeqDataset:\n","        n_obs = self.n_obs[type_path]\n","        max_target_length = self.target_lens[type_path]\n","        dataset = Seq2SeqDataset(\n","            self.tokenizer,\n","            type_path=type_path,\n","            n_obs=n_obs,\n","            max_target_length=max_target_length,\n","            **self.dataset_kwargs,\n","        )\n","        return dataset\n","\n","    def get_dataloader(self, type_path: str, batch_size: int, shuffle: bool = False) -> DataLoader:\n","        dataset = self.get_dataset(type_path)\n","\n","        dataloader = DataLoader(\n","            dataset,\n","            batch_size=batch_size,\n","            collate_fn=dataset.collate_fn,\n","            shuffle=shuffle,\n","            num_workers=self.num_workers,\n","        )\n","        return dataloader\n","\n","    def train_dataloader(self) -> DataLoader:\n","        dataloader = self.get_dataloader(\"train\", batch_size=self.hparams.train_batch_size, shuffle=True)\n","        return dataloader\n","\n","    def val_dataloader(self) -> DataLoader:\n","        return self.get_dataloader(\"val\", batch_size=self.hparams.eval_batch_size)\n","\n","    def test_dataloader(self) -> DataLoader:\n","        return self.get_dataloader(\"test\", batch_size=self.hparams.eval_batch_size)\n","    "]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["hparams = parse_sh_args(\"fine_tune_rag.sh\")\n","QAModule = GenerativeQAModule(hparams)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
