{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Dataset generation\n",
    "\n",
    "In this notebook, we will generate a dataset from Wikipedia articles. We will use the `wikipedia` library to download the articles and then we will extract the text from them. We will only consider the most popular articles in English to prevent our dataset from being too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wiki\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import csv\n",
    "import faiss\n",
    "import torch\n",
    "from datasets import load_dataset, Features, Value, Sequence, Dataset\n",
    "from functools import partial\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user agent\n",
    "wiki_wiki = wiki.Wikipedia(\"RUG NLP Q&A\", 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Popular Articles\n",
    "\n",
    "We will refer to the Popular pages list on wikipedia to get the most popular articles. We see that there are 1079 links to other pages. We will extract the links and then download the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1079 popular pages\n"
     ]
    }
   ],
   "source": [
    "# Get all pages mentioned in this url https://en.wikipedia.org/wiki/Wikipedia:Popular_pages\n",
    "popular_pages = wiki_wiki.page(\"Wikipedia:Popular_pages\")\n",
    "popular_pages_links = popular_pages.links\n",
    "\n",
    "print(f\"Found {len(popular_pages_links)} popular pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conform to the medallion standard for the dataset. Thus, we will have raw data, bronze data, silver data, and gold data. The raw data will be the text of the articles. The bronze data will be the processed text of the articles. The silver data will a json file containing the text and the title of the article. The gold data will be the embeddings of each sentence in the article.\n",
    "\n",
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801479b76d0345238bfcba3e85192d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the text of all the pages as specified in the links above\n",
    "# Save the text in a file with the name of the page as the file name, save it in data/raw\n",
    "for page in tqdm(popular_pages_links):\n",
    "    page = wiki_wiki.page(page)\n",
    "\n",
    "    # If the file already exists, skip it\n",
    "    if os.path.exists(f\"data/raw/{page.title}.txt\"):\n",
    "        continue\n",
    "    \n",
    "    # Account for pages with / in their name by replacing it with _\n",
    "    file_name = page.title.replace(\"/\", \"_\")\n",
    "\n",
    "    # replace ? with _ in the file name\n",
    "    file_name = file_name.replace(\"?\", \"_\")\n",
    "\n",
    "    with open(f\"data/raw/{file_name}.txt\", \"w\") as f:\n",
    "        f.write(page.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Data\n",
    "\n",
    "We will remove the references, external links and see also sections from the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d31ed05c5143e7b93d1dca054d9700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data from the raw files, remove the see also, references, external links and notes sections. Then save it in data/bronze\n",
    "for file in tqdm(os.listdir(\"data/raw/\")):\n",
    "    with open(f\"data/raw/{file}\", \"r\") as f:\n",
    "        dataset = f.read()\n",
    "\n",
    "    # Remove the see also, references, external links and notes sections\n",
    "    dataset = dataset.split(\"See also\")[0]\n",
    "    dataset = dataset.split(\"References\")[0]\n",
    "    dataset = dataset.split(\"External links\")[0]\n",
    "    dataset = dataset.split(\"Notes \")[0]\n",
    "\n",
    "    with open(f\"data/bronze/{file}\", \"w\") as f:\n",
    "        f.write(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Data\n",
    "\n",
    "We will create a csv file containing the title and the text of the article. The csv format is required for use with the datasets library. We use tab as the delimiter to prevent any issues with commas in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfdfe207bfe473b855c6d883eb1ff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the text of all the pages in data/bronze and store it in a csv file with the title of the page as the first column and the text as the second column\n",
    "with open(\"data/silver/data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\", lineterminator=\"\\n\")\n",
    "\n",
    "    writer.writerow([\"title\", \"text\"])\n",
    "\n",
    "    for file in tqdm(os.listdir(\"data/bronze/\")):\n",
    "        with open(f\"data/bronze/{file}\", \"r\") as f:\n",
    "            dataset = f.read().replace(\"\\n\", \" \")\n",
    "            dataset = dataset.replace(\"\\t\", \" \")\n",
    "        \n",
    "        writer.writerow([file.replace(\".txt\", \"\"), dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Data\n",
    "\n",
    "We will generate the index using faiss and a context encoder.\n",
    "\n",
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_len=100):\n",
    "    \"\"\"\n",
    "    Split the text using \" \" into chunks of max_len words\n",
    "    \"\"\"\n",
    "    text = text.split(\" \")\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), max_len):\n",
    "        chunks.append(\" \".join(text[i:i+max_len]))\n",
    "        \n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Split the dataset into chunks of 100 words, returns a dictionary with the title and the text\n",
    "    \"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(dataset[\"title\"], dataset[\"text\"]):\n",
    "\n",
    "        if text is None:\n",
    "            continue\n",
    "\n",
    "        chunks = split_text(text)\n",
    "        for chunk in chunks:\n",
    "            titles.append(title)\n",
    "            texts.append(chunk)\n",
    "\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the dataset into 80567 chunks\n",
      "The hyphen-minus symbol - is the form of hyphen most commonly used in digital documents. On most keyboards, it is the only character that resembles a minus sign or a dash so it is also used for these. The name hyphen-minus derives from the original ASCII standard, where it was called hyphen–(minus). The character is referred to as a hyphen, a minus sign, or a dash according to the context where it is being used.  Description In early typewriters and character encodings, a single key/code was almost always used for hyphen, minus, various dashes, and strikethrough, since they all\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and split the text into chunks of 100 words\n",
    "dataset = load_dataset(\"csv\", data_files=\"data/silver/data.csv\", delimiter=\"\\t\")\n",
    "chunked_dataset = dataset.map(split_dataset, batched=True)\n",
    "print(f\"Split the dataset into {len(chunked_dataset['train']['title'])} chunks\")\n",
    "print(chunked_dataset['train'][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the embeddings\n",
    "We will use the albert-base-v2 model to generate the embeddings due to its speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adec7b9ca6f140d28017d770c7149714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer, we will not train the model yet but use it to encode the text\n",
    "encoder = SentenceTransformer('sentence-transformers/paraphrase-albert-base-v2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get embedding shape from config\n",
    "embedding_shape = encoder.encode(\"test\").shape\n",
    "print(f\"Embedding shape: {embedding_shape}\")\n",
    "\n",
    "new_features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"), \n",
    "        \"title\": Value(\"string\"), \n",
    "        \"embeddings\": Sequence(Value(\"float32\"))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce this amount if you run out of memory\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def embed(dataset, encoder):\n",
    "    \"\"\"\n",
    "    Embed the text using the encoder and tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    input = list(zip(dataset[\"text\"], dataset[\"title\"]))\n",
    "    embeddings = encoder.encode(input, show_progress_bar=True, device=device, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\"title\": dataset[\"title\"], \"text\": dataset[\"text\"], \"embeddings\": embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f49ab0b40f4c9893c24e32a4ef1c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embed the text without mapping\n",
    "embedded_dataset = embed(chunked_dataset[\"train\"], encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the index\n",
    "\n",
    "We will use the faiss library to create the index with the inner product as the similarity measure. This is the same measure used in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d757b31e07457381b1af1cc3899911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to huggingface dataset\n",
    "index_dataset = Dataset.from_dict(embedded_dataset, features=new_features)\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_shape[0])\n",
    "index_dataset.add_faiss_index(\"embeddings\", custom_index=index)\n",
    "\n",
    "# Create gold folder if it does not exist\n",
    "if not os.path.exists(\"data/gold\"):\n",
    "    os.makedirs(\"data/gold\")\n",
    "\n",
    "# Save the dataset with the faiss index\n",
    "index_dataset.get_index(\"embeddings\").save(\"data/gold/index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 231.4854736328125\n",
      "Title: Netherlands\n",
      "Text: The Netherlands, informally Holland, is a country located in northwestern Europe with overseas territories in the Caribbean. It is the largest of the four constituent countries of the Kingdom of the Netherlands. The Netherlands consists of twelve provinces; it borders Germany to the east and Belgium to the south, with a North Sea coastline to the north and west. It shares maritime borders with the United Kingdom, Germany, and Belgium. The official language is Dutch, with West Frisian as a secondary official language in the province of Friesland. Dutch, English, and Papiamento are official in the Caribbean territories.Netherlands literally means\n",
      "\n",
      "Distance: 217.090576171875\n",
      "Title: Kingdom of the Netherlands\n",
      "Text: The Kingdom of the Netherlands (Dutch: Koninkrijk der Nederlanden, pronounced [ˈkoːnɪŋkrɛik dɛr ˈneːdərlɑndə(n)] ), commonly known simply as the Netherlands, is a sovereign state consisting of a collection of constituent territories united under the monarch of the Netherlands, who functions as head of state. The realm is not a federation; it is a unitary monarchy with its largest subdivision, the eponymous Netherlands, predominantly located in Western Europe and with several smaller island territories located in the Caribbean. The four subdivisions of the Kingdom — the Netherlands, Aruba, Curaçao, and Sint Maarten — are constituent countries (landen in Dutch; singular: land)\n",
      "\n",
      "Distance: 207.81857299804688\n",
      "Title: Amsterdam\n",
      "Text: Amsterdam ( AM-stər-dam, UK also  AM-stər-DAM, Dutch: [ˌɑmstərˈdɑm] ; literally, \"The Dam on the River Amstel\") is the capital and most populated city of the Netherlands. It has a population of 921,402 within the city proper, 1,457,018 in the urban area and 2,480,394 in the metropolitan area. Located in the Dutch province of North Holland, Amsterdam is colloquially referred to as the \"Venice of the North\", for its large number of canals, now a UNESCO World Heritage Site.Amsterdam was founded at the mouth of the Amstel River that was dammed to control flooding. Originally a small fishing village in\n",
      "\n",
      "Distance: 198.26055908203125\n",
      "Title: Netherlands\n",
      "Text: 5th largest metropolitan area in Europe. According to Dutch Central Statistics Bureau, in 2015, 28 per cent of the Dutch population had a spendable income above 45,000 euros (which does not include spending on health care or education).  Language The official language of the Netherlands is Dutch, which is spoken by the vast majority of inhabitants. The dialects most spoken in the Netherlands are the Brabantian-Hollandic dialects.Besides Dutch, West Frisian is recognised as a second official language in the northern province of Friesland (Fryslân in West Frisian). West Frisian has a formal status for government correspondence in that province.\n",
      "\n",
      "Distance: 194.129150390625\n",
      "Title: Netherlands\n",
      "Text: Dutch location gives it prime access to markets in the United Kingdom and Germany, with the Port of Rotterdam being the largest port in Europe. Other important parts of the economy are international trade, banking and transport. The Netherlands successfully addressed the issue of public finances and stagnating job growth long before its European partners. Amsterdam is the 5th-busiest tourist destination in Europe, with more than 4.2 million international visitors. Since the enlargement of the EU, large numbers of migrant workers have arrived in the Netherlands from Central and Eastern Europe.The Netherlands continues to be one of the leading European\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOP_K = 5\n",
    "text = \"What is the capital of the Netherlands?\"\n",
    "\n",
    "# Get the embeddings for the question\n",
    "embeddings = encoder.encode(text, device=device)\n",
    "\n",
    "# Search the faiss index for the most similar embeddings\n",
    "D, I = index_dataset.get_index(\"embeddings\").search(embeddings, TOP_K)\n",
    "\n",
    "# Get the text, titles and distances of the most similar embeddings\n",
    "for i, (distance, index) in enumerate(zip(D, I)):\n",
    "    print(f\"Distance: {distance}\")\n",
    "    print(f\"Title: {index_dataset['title'][index]}\")\n",
    "    print(f\"Text: {index_dataset['text'][index]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
